# Tokens
Text-based models, like GPT, work with tokens (not words). On average, a token is about 4 characters.

Ex: "I like chicken sammies"

GPT-3.5 would classify this as 6 tokens: "I", "like", "chicken", "s", "amm", "ies

To figure how how many tokens a sentence may have, you can use the GPT Tokenizer tool: https://platform.openai.com/tokenizer

